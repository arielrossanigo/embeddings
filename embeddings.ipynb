{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ariel Rossanigo**\n",
    "\n",
    "git clone git@github.com:arielrossanigo/embeddings.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quien soy?\n",
    "\n",
    "* Ariel Rossanigo\n",
    "* Artificial Intelligence teacher at UCSE-DAR\n",
    "* Developer, Data Scientist\n",
    "* Co-Founder of Bloom AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Embeddings \n",
    "\n",
    "<div><img src=\"./imgs/mnist_embedding.png\" width=\"40%\" style=\"float: right; margin: 10px;\" align=\"right\"></div>\n",
    "\n",
    "\n",
    "#### *Mapping of a discrete (categorical) variable to a vector of continuous numbers preserving some meaningful properties*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLP & Word2Vec\n",
    "\n",
    "#### How we can preprocess words in order to solve NLP tasks?\n",
    "\n",
    "* One hot encoding \n",
    "\n",
    "* Word embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### One hot encoding \n",
    "\n",
    "\n",
    "* Each word is a vector of length *V*; where *V* is the vocabulary size\n",
    "* Simple\n",
    "* Huge vector when *V* is high (and tipically it is)\n",
    "* Loose *semantic* meaning\n",
    "\n",
    "<div><img src=\"./imgs/one_hot_encoding.png\" width=\"35%\" style=\"float: right; margin: 10px;\"></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Word2Vec\n",
    "\n",
    "* Maps each word to a low dimentional vector of real numbers\n",
    "* Preserve semantic relationships\n",
    "* Assumes that words that share similar contexts will be similar to each other\n",
    "\n",
    "* In the original paper [1], autors present 2 models: Continuos Bag Of Words and Skip-Gram\n",
    "\n",
    "\n",
    "<div><img src=\"./imgs/word2vec.png\" height=\"20%\" ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Basics\n",
    "\n",
    "* The basic idea is to use context of words \n",
    "\n",
    "<div><img src=\"./imgs/context_window.png\" width=\"50%\" ></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Basics\n",
    "\n",
    "* Two different ways of looking at the problem\n",
    "\n",
    "<div><img src=\"./imgs/skipgram_nn.png\" width=\"40%\" style=\"float: left; margin: 10px;\"></div>\n",
    "<div><img src=\"./imgs/skipgram_dual.png\" width=\"50%\" style=\"float: right; margin: 10px;\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec SkipGramNegativeSampling\n",
    "\n",
    "* An optimization in the second approach\n",
    "* Instead of having only pair (w, c) it creates pairs (w, c) = 1 | (w, nc) = 0 and makes a classifier\n",
    "\n",
    "* Another improvements are listed in original papers [2] and a *simpler* explanation can be found here [3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How can I obtain the \"embedding\"?\n",
    "\n",
    "* In approach one, is the output of W1\n",
    "* In the second approach, is the output of *target_embedding_layer* or a combination with *context_embedding_layer*, or maybe we can decide to have the exactly same weights using a siamese network.\n",
    "\n",
    "* Another improvements are listed in original papers [2] and a *simpler* explanation can be found here [3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some astonish facts with semantics...\n",
    "\n",
    "* Before Word2Vec the semantics relationships were simple, for instance *France* is similar to *Italy* and another countries names...\n",
    "\n",
    "* In Word2Vec paper they show that performing some algebraic operation they can answer some question:\n",
    "\n",
    "    * vector(biggest) - vector(big) + vector(small) gives a vector where the closest word was \"Smallest\"\n",
    "    * vector(France) - vector(Paris) + vector(Germany) ~= \"Berlin\"    \n",
    "    * vector(possibly) - vector(impossibly) + vector(ethical) ~= \"unethical\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img src=\"./imgs/embeddings_can_fail.png\" width=\"80%\" style=\"float: middle; margin: 10px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other uses of word embeddings: Prod2Vec[4]\n",
    "\n",
    "* Same ideas, but instead of sentences composed of words we have orders composed of products...\n",
    "* With dual vector approach we can find similar products and complementary products, useful for recommendations like:\n",
    "    * if you want to buy this... another options are these\n",
    "    * if you bought this... maybe you would like to buy these things too\n",
    "\n",
    "<div><img src=\"./imgs/skipgram_dual.png\" width=\"80%\" style=\"float: middle; margin: 10px;\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's see some code...\n",
    "\n",
    "We will use *gensim* library that provides word2vec implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:30:03.140659Z",
     "start_time": "2021-05-24T21:30:03.138280Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:30:03.446951Z",
     "start_time": "2021-05-24T21:30:03.144198Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:30:04.564037Z",
     "start_time": "2021-05-24T21:30:03.452375Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet('sample_of_orders.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:30:04.577357Z",
     "start_time": "2021-05-24T21:30:04.567177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1816652, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:30:17.657150Z",
     "start_time": "2021-05-24T21:30:04.579373Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets group orders as lists of products\n",
    "sentences = df.groupby('txn_id').product_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:30:17.664972Z",
     "start_time": "2021-05-24T21:30:17.658781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "txn_id\n",
       "1000003160592747140146510920181013246      [ball park cheese franks            14 oz, pri...\n",
       "10000043206055511401520209201903101016     [private label - fz seafood, private label - c...\n",
       "1000006510601821720148820220190126466      [private label - internal analgesics, private ...\n",
       "10000428105966328001159720181127959524     [bigelow bnft, private label - butter/butter b...\n",
       "100004402061220148011101220190526441034    [lil potato savoury herb, bubba burger origina...\n",
       "Name: product_name, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:30:18.126869Z",
     "start_time": "2021-05-24T21:30:17.666578Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets get the arrays\n",
    "as_arrays = sentences.map(list).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:30:18.134673Z",
     "start_time": "2021-05-24T21:30:18.130378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['ball park cheese franks            14 oz', 'private label - rfg salad/coleslaw', 'private label - soup', 'private label - dough/biscuit dough - rfg', 'bar-s smokehouse franks']),\n",
       "       list(['private label - fz seafood', 'private label - carbonated beverages', 'jack link beef orginal jerky     2.85 oz', 'jlytm blast o butter 10.5 oz', 'edys sc light vanilla bean cup'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_arrays[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:30:18.423716Z",
     "start_time": "2021-05-24T21:30:18.136470Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ariel/Envs/embeddings/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "- window (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "- min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "- hs ({0, 1}, optional) – If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "- negative (int, optional) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "- ns_exponent (float, optional) – The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper. More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that other values may perform better for recommendation applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:31:57.886515Z",
     "start_time": "2021-05-24T21:31:25.541348Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets train the word2vec\n",
    "window_size = 20\n",
    "epochs = 5\n",
    "embedding_size = 100\n",
    "\n",
    "#Drop infrequent items in dataset\n",
    "min_count = 5  \n",
    "number_of_negative_samples = 7\n",
    "ns_exponent = 0\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences=as_arrays,\n",
    "    sg=1,  # use skipgram\n",
    "    vector_size=embedding_size, \n",
    "    window=window_size, \n",
    "    min_count=min_count, \n",
    "    workers=4,\n",
    "    hs=0,\n",
    "    negative=number_of_negative_samples,\n",
    "    ns_exponent=ns_exponent, \n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:31:57.890668Z",
     "start_time": "2021-05-24T21:31:57.888325Z"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:31:57.905774Z",
     "start_time": "2021-05-24T21:31:57.892553Z"
    }
   },
   "outputs": [],
   "source": [
    "# product = 'edys sc light vanilla bean cup'\n",
    "product = 'coke classic  nr 20 oz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:31:57.922624Z",
     "start_time": "2021-05-24T21:31:57.907496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.37786067,  0.10014538, -0.3283386 , -0.2734145 ,  0.32060197,\n",
       "       -0.5418468 , -0.05063178,  0.08003001,  0.11638877, -0.20754491,\n",
       "       -0.11380889, -0.30682042,  0.12010448,  0.1645465 ,  0.51134473,\n",
       "        0.18624924,  0.05772192,  0.19505486, -0.3844798 , -0.64751995,\n",
       "       -0.1735677 , -0.47491857, -0.44679746, -0.09748479, -0.4362577 ,\n",
       "        0.19383126, -0.40425456, -0.01894782, -0.1811316 , -0.10646037,\n",
       "        0.49161908, -0.13181087, -0.12184609, -0.18782769, -0.16105498,\n",
       "        0.05481014, -0.18293212,  0.19886328, -0.04804575, -0.42942217,\n",
       "        0.01384899,  0.06187457, -0.25201553, -0.18278848,  0.15408042,\n",
       "       -0.00554037, -0.3577863 , -0.39891836,  0.12177671,  0.1775741 ,\n",
       "        0.14827101, -0.12403344, -0.14512165, -0.36569324,  0.09821078,\n",
       "       -0.26531336,  0.2678501 ,  0.05258592, -0.28740278, -0.28290296,\n",
       "       -0.27937248, -0.07517467,  0.17154448, -0.09534959, -0.32005057,\n",
       "        0.28445038,  0.25396556,  0.47513235, -0.51146364,  0.15401137,\n",
       "        0.38686287,  0.29210472,  0.16056818, -0.00602955,  0.36876267,\n",
       "        0.2773545 , -0.05995991,  0.09696642, -0.04769377, -0.06825668,\n",
       "       -0.42728496, -0.16024832,  0.1213026 ,  0.5013778 , -0.1317611 ,\n",
       "       -0.056416  ,  0.3750762 ,  0.6470526 ,  0.04056923, -0.08483353,\n",
       "        0.339482  ,  0.09584592,  0.21952051,  0.06677189,  0.38204247,\n",
       "        0.13530418,  0.25634044, -0.06461878, -0.13463582, -0.04405804],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors[product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:31:57.940109Z",
     "start_time": "2021-05-24T21:31:57.924893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coke sprite nr 20 oz', 0.9018566608428955),\n",
       " ('dr ppr nr 20 oz', 0.8920454382896423),\n",
       " ('pepsi nr 20 oz', 0.8843024373054504),\n",
       " ('mt dew nr 20 oz', 0.8816001415252686),\n",
       " ('m&m pnut choc candy ks 1 ct', 0.8590784668922424),\n",
       " ('snkers almond mixed shpr 1 ct', 0.854461669921875),\n",
       " ('slmjim jim giant .97 oz', 0.8429577946662903),\n",
       " ('snkers king size 2 - piece bar 1 ct', 0.8365886807441711),\n",
       " ('reese pnt btr cup kng sz 1 ct', 0.8359562754631042),\n",
       " ('fanta orange 20 oz', 0.829744279384613)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(product, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:31:58.303309Z",
     "start_time": "2021-05-24T21:31:57.941812Z"
    }
   },
   "outputs": [],
   "source": [
    "products = []\n",
    "vectors = []\n",
    "for product in df.product_name.unique():\n",
    "    try:\n",
    "        vectors.append(word_vectors[product])\n",
    "        products.append(product)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:31:58.308678Z",
     "start_time": "2021-05-24T21:31:58.304423Z"
    }
   },
   "outputs": [],
   "source": [
    "df_products = pd.DataFrame({'product_name': products})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:31:59.377075Z",
     "start_time": "2021-05-24T21:31:58.310480Z"
    }
   },
   "outputs": [],
   "source": [
    "df_vectors = pd.DataFrame(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T21:32:03.123855Z",
     "start_time": "2021-05-24T21:31:59.378718Z"
    }
   },
   "outputs": [],
   "source": [
    "df_products.to_csv('products_metadata.tsv', sep='\\t', index=False, header=False)\n",
    "df_vectors.to_csv('products_embeddings.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now data can be loaded into https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More uses for a future talk\n",
    "\n",
    "* Encoding categorical variables Cat2Vec: TL;DR: use a supervise method to train embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Gracias! Preguntas?\n",
    "\n",
    "\n",
    "<div style=\"float: left;\"><img src=\"../common/imgs/man-qmark.jpg\" width=\"300\" align=\"middle\"></div> \n",
    "\n",
    "<div>\n",
    "<div>\n",
    "  <img src=\"../common/imgs/gmail-1162901_960_720.png\" style=\"width: 30px; float: left; vertical-align:middle; margin: 0px;\">\n",
    "  <span style=\"line-height:30px; vertical-align:middle; margin-left: 10px;\">arielrossanigo@gmail.com</span>\n",
    "</div>\n",
    "<div>\n",
    "  <img src=\"../common/imgs/twitter-312464_960_720.png\" style=\"width: 30px; float: left; vertical-align:middle; margin: 0px;\">\n",
    "  <span style=\"line-height:30px; vertical-align:middle; margin-left: 10px;\">@arielrossanigo</span>\n",
    "</div>\n",
    "<div>\n",
    "  <img src=\"../common/imgs/github-154769__340.png\" style=\"width: 30px; float: left; vertical-align:middle; margin: 0px;\">\n",
    "  <span style=\"line-height:30px; vertical-align:middle; margin-left: 10px;\">https://github.com/arielrossanigo</span>\n",
    "</div>\n",
    "<div>\n",
    "  <img src=\"../common/imgs/Linkedin_icon.svg\" style=\"width: 30px; float: left; vertical-align:middle; margin: 0px;\">\n",
    "  <span style=\"line-height:30px; vertical-align:middle; margin-left: 10px;\">https://www.linkedin.com/in/arielrossanigo/</span>\n",
    "</div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Efficient Estimation of Word Representations in Vector Space ( https://arxiv.org/abs/1301.3781)\n",
    "\n",
    "[2] Distributed Representations of Words and Phrases and their Compositionality (https://arxiv.org/abs/1310.4546)\n",
    "\n",
    "[3] word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method (https://arxiv.org/abs/1402.3722)\n",
    "\n",
    "[4] E-commerce in Your Inbox: Product Recommendations at Scale ( https://arxiv.org/abs/1606.07154)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
